{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import HTML\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'  # display full outputs in Jupyter\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than training our own word embeddings, a very expensive operation, we can use word embeddings that were trained on a large corpus of words. The hope is that these embeddings will generalize from the training corpus to our needs.\n",
    "\n",
    "This code downloads 100-dimensional word embeddings if you don't already have them. There are a number of different pre-trained word embeddings you can find from [Stanford online](https://nlp.stanford.edu/data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import get_file\n",
    "from os.path import realpath, dirname, exists, join\n",
    "ROOT_DIR = dirname(realpath('.'))\n",
    "DATA_DIR = join(ROOT_DIR, 'data')\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download word embeddings from stanford NLP laab\n",
    "glove_vectors = join(DATA_DIR, 'glove.6B.zip')\n",
    "\n",
    "if not os.path.exists(glove_vectors):\n",
    "    glove_vectors = get_file('glove.6B.zip',\n",
    "                             'http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    os.system(f'unzip {glove_vectors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 101)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in unzipped files\n",
    "glove_vectors = join(DATA_DIR, 'glove.6B.100d.txt')\n",
    "\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate into words and vectors\n",
    "words = glove[:, 0]\n",
    "vectors = glove[:, 1:].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete glove to save memory\n",
    "del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21843  ,  0.022696 , -0.062105 , -0.25557  , -0.2222   ,\n",
       "        0.75584  , -0.58643  , -0.3236   ,  0.0036797, -0.52816  ,\n",
       "       -0.18682  ,  0.16995  ,  0.38306  ,  0.26499  , -0.081493 ,\n",
       "       -0.85389  ,  0.078729 ,  0.55321  , -0.94035  , -0.046033 ,\n",
       "        0.25873  , -0.51662  ,  0.17764  , -0.54664  , -0.64107  ,\n",
       "       -0.71131  , -0.66956  , -0.16875  ,  0.25056  , -0.073421 ,\n",
       "        0.742    ,  0.21894  , -0.60056  , -0.66511  ,  0.87591  ,\n",
       "       -0.43214  , -0.16481  ,  0.15383  , -0.4014   , -0.17786  ,\n",
       "       -0.57662  ,  0.038627 , -0.1438   , -0.21172  ,  0.023644 ,\n",
       "       -0.38741  , -0.091636 ,  0.80288  , -0.56324  , -0.7643   ,\n",
       "       -0.15529  ,  0.40837  ,  0.023216 ,  1.6483   , -0.36147  ,\n",
       "       -1.8609   ,  0.40398  , -0.41986  ,  1.5969   ,  0.2239   ,\n",
       "       -0.26619  ,  1.3771   , -0.43608  ,  0.1363   ,  0.62087  ,\n",
       "        0.33013  ,  0.90322  ,  0.22929  , -0.072946 , -0.16841  ,\n",
       "       -0.13554  ,  0.0075493, -0.2734   , -0.25576  ,  0.061146 ,\n",
       "       -0.13276  , -0.21993  , -0.29111  , -0.20789  , -0.1394   ,\n",
       "        0.20315  ,  0.11451  , -0.68641  ,  0.63453  , -1.4408   ,\n",
       "       -0.301    , -0.42911  ,  0.48833  , -0.91329  ,  0.037517 ,\n",
       "       -0.35243  , -0.31124  ,  0.39479  , -0.48404  , -0.58556  ,\n",
       "       -0.57038  , -0.39338  ,  0.26525  ,  0.95566  ,  0.084452 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'themselves'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[1000]\n",
    "words[1000]\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only keep the words with embeddings that appear in our vocabulary. For words that are in our vocabulary but don't have an embedding, they will be represented as all 0s. We can address that by training our own embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary for word look up table\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "word_lookup['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_abstract</th>\n",
       "      <th>patent_date</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
       "      <td>1996-07-09</td>\n",
       "      <td>5535303</td>\n",
       "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" This invention is a novel high-speed neural ...</td>\n",
       "      <td>1993-10-19</td>\n",
       "      <td>5255349</td>\n",
       "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An optical information processor for use as a ...</td>\n",
       "      <td>1995-01-17</td>\n",
       "      <td>5383042</td>\n",
       "      <td>3 layer liquid crystal neural network with out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>6169981</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A method and system for intelligent control of...</td>\n",
       "      <td>2003-06-17</td>\n",
       "      <td>6581048</td>\n",
       "      <td>3-brain architecture for an intelligent decisi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     patent_abstract patent_date  \\\n",
       "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  1996-07-09   \n",
       "1  \" This invention is a novel high-speed neural ...  1993-10-19   \n",
       "2  An optical information processor for use as a ...  1995-01-17   \n",
       "3  A method and system for intelligent control of...  2001-01-02   \n",
       "4  A method and system for intelligent control of...  2003-06-17   \n",
       "\n",
       "  patent_number                                       patent_title  \n",
       "0       5535303        \"\"\"Barometer\"\" neuron for a neural network\"  \n",
       "1       5255349  \"Electronic neural network for solving \"\"trave...  \n",
       "2       5383042  3 layer liquid crystal neural network with out...  \n",
       "3       6169981  3-brain architecture for an intelligent decisi...  \n",
       "4       6581048  3-brain architecture for an intelligent decisi...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3522"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create word and tokens from our own copora. \n",
    "import pandas as pd\n",
    "data = pd.read_csv('../data/neural_network_patent_query.csv', parse_dates=['patent_date'])\n",
    "data.head()\n",
    "\n",
    "# Extract abstracts\n",
    "original_abstracts = list(data['patent_abstract'])\n",
    "len(original_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from utils import make_sequences, format_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_abstracts = list(data['patent_abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [format_sequence(a) for a in original_abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" This invention is a novel high-speed neural network based processor for solving the \"\"traveling salesman\"\" and other global optimization problems . It comprises a novel hybrid architecture employing a binary synaptic array whose embodiment incorporates the fixed rules of the problem , such as the number of cities to be visited . The array is prompted by analog voltages representing variables such as distances . The processor incorporates two interconnected feedback networks , each of which solves part of the problem independently and simultaneously , yet which exchange information dynamically . \"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11695 unique words.\n",
      "There are 293001 sequences.\n"
     ]
    }
   ],
   "source": [
    "word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels = make_sequences(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 1224 words without pre-trained embeddings.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for i, word in enumerate(word_idx.keys()):\n",
    "    # Look up the word embedding\n",
    "    vector = word_lookup.get(word, None)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i + 1, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "print(f'There were {not_found} words without pre-trained embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.enable()\n",
    "del vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word is represented by 100 numbers with a number of words that can't be found. We can find the closest words to a given word in embedding space using the cosine distance. This requires first normalizing the vectors to have a magnitude of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and convert nan to 0\n",
    "embedding_matrix = embedding_matrix / \\\n",
    "    np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(query, embedding_matrix, word_idx, idx_word, n=10):\n",
    "    \"\"\"Find closest words to a query word in embeddings\"\"\"\n",
    "\n",
    "    idx = word_idx.get(query, None)\n",
    "    # Handle case where query is not in vocab\n",
    "    if idx is None:\n",
    "        print(f'{query} not found in vocab.')\n",
    "        return\n",
    "    else:\n",
    "        vec = embedding_matrix[idx]\n",
    "        # Handle case where word doesn't have an embedding\n",
    "        if np.all(vec == 0):\n",
    "            print(f'{query} has no pre-trained embedding.')\n",
    "            return\n",
    "        else:\n",
    "            # Calculate distance between vector and all others\n",
    "            dists = np.dot(embedding_matrix, vec)\n",
    "\n",
    "            # Sort indexes in reverse order\n",
    "            idxs = np.argsort(dists)[::-1][:n]\n",
    "            sorted_dists = dists[idxs]\n",
    "            closest = [idx_word[i] for i in idxs]\n",
    "\n",
    "    print(f'Query: {query}\\n')\n",
    "    max_len = max([len(i) for i in closest])\n",
    "    # Print out the word and cosine distances\n",
    "    for word, dist in zip(closest, sorted_dists):\n",
    "        print(f'Word: {word:15} Cosine Similarity: {round(dist, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10463, 11531, 11042, ...,   428,  8083,    71])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = word_idx.get('invention', None)\n",
    "vec = embedding_matrix[idx]\n",
    "dists = np.dot(embedding_matrix, vec)\n",
    "\n",
    "np.argsort(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.argsort(dists)[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dists = dists[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest = [idx_word[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: invention       Cosine Similarity: 1.0\n",
      "Word: invented        Cosine Similarity: 0.7513\n",
      "Word: technique       Cosine Similarity: 0.6008\n",
      "Word: concept         Cosine Similarity: 0.5966\n",
      "Word: introduction    Cosine Similarity: 0.5679\n",
      "Word: method          Cosine Similarity: 0.559\n",
      "Word: design          Cosine Similarity: 0.5575\n",
      "Word: patent          Cosine Similarity: 0.555\n",
      "Word: experiment      Cosine Similarity: 0.5463\n",
      "Word: modern          Cosine Similarity: 0.5449\n"
     ]
    }
   ],
   "source": [
    "for word, dist in zip(closest, sorted_dists):\n",
    "    print(f'Word: {word:15} Cosine Similarity: {round(dist, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: design\n",
      "\n",
      "Word: design          Cosine Similarity: 1.0\n",
      "Word: designs         Cosine Similarity: 0.8617\n",
      "Word: designed        Cosine Similarity: 0.7899\n",
      "Word: architecture    Cosine Similarity: 0.7741\n",
      "Word: model           Cosine Similarity: 0.735\n",
      "Word: architectural   Cosine Similarity: 0.7183\n",
      "Word: concept         Cosine Similarity: 0.7056\n",
      "Word: developed       Cosine Similarity: 0.6946\n",
      "Word: models          Cosine Similarity: 0.6872\n",
      "Word: structure       Cosine Similarity: 0.6872\n"
     ]
    }
   ],
   "source": [
    "find_closest('design', embedding_matrix, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn has no pre-trained embedding.\n"
     ]
    }
   ],
   "source": [
    "find_closest('dnn', embedding_matrix, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepvrm",
   "language": "python",
   "name": "deepvrm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
